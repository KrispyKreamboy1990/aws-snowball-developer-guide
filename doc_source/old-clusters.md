--------

This guide is for the Snowball Edge \(100 TB of storage space\)\. If you are looking for documentation for the Snowball, see the [AWS Snowball User Guide](http://docs.aws.amazon.com/snowball/latest/ug/whatissnowball.html)\.

--------

# Clustering Overview<a name="old-clusters"></a>

**Note**  
In January 2018, there was a feature update to for clusters, making them leaderless\. The cluster update is backwards compatible with older clusters\. This is the original cluster documentation for the Snowball Edge\. If you're looking for the updated content, see [ Using an AWS Snowball Edge Cluster  Learn to use an AWS Snowball cluster—find in this topic conceptual, monitoring, administration, and walkthrough information for clusters of Snowball Edge devices\.   A cluster is a logical grouping of AWS Snowball Edge devices, in groups of 5 to 10 devices\. A cluster is created as a single job, which offers increased durability and storage capacity\. In the following topic, you can find information about Snowball Edge clusters\. This information includes conceptual, usage, and administrative information, in addition to walkthroughs for common Snowball Edge procedures\.  In January 2018, there was a feature update for clusters, making them leaderless\. The cluster update is backward\-compatible with older clusters\. However, if you're looking for the original cluster documentation, see [Additional Information for Snowball Edge](appendices.md)\.    Clustering Overview  For the AWS Snowball service, a cluster is a collective of Snowball Edges, used as a single logical unit, for local storage and compute purposes\. A cluster offers two primary benefits over a standalone Snowball Edge for local storage and compute purposes:   **Increased Durability** – The data stored in a cluster of Snowball Edge appliances enjoys increased data durability over a single device\. In addition, the data on the cluster remains as safe and viable as it was previously, despite possible Snowball Edge outages in the cluster\. Clusters can withstand the loss of two nodes before the data is in danger\. You can also add or replace nodes\.   **Increased Storage** – The total available storage is 45 terabytes of data per node in the cluster\. Thus, in a five\-node cluster there are 225 terabytes of available storage space\. In contrast, there are about 82 terabytes of available storage space in a standalone Snowball Edge\. Clusters that have more than five nodes have even more storage space\.   A cluster of Snowball Edge devices is made of leaderless nodes\. Any node can write data to and read data from the entire cluster, and all nodes are capable of performing the behind\-the\-scenes management of the cluster\.  Snowball Edge Cluster Quorums  A quorum represents the minimum number of Snowball Edge devices in a cluster that must be communicating with each other to maintain some level of operation\. There are two levels of quorum for Snowball Edge clusters—a read/write quorum and a read quorum\. Let's say you upload your data to a cluster of Snowball Edge devices\. With all devices healthy, you have a read/write quorum for your cluster\. If one of those nodes goes offline, you reduce the operational capacity of the cluster\. However, you can still read and write to the cluster\. In that sense, with the cluster operating all but one node, the cluster still has a read/write quorum\. If two nodes in your cluster are down, any additional or ongoing write operations fail\. But any data that was successfully written to the cluster can be accessed and read\. This is called a read quorum\. Finally, suppose that a third node loses power\. Then the cluster is offline, and the data in the cluster is unavailable\. You might be able fix this, or the data might be permanently lost, depending on the severity of the event\. If it is a temporary external power event, and you can power the three Snowball Edges back on and unlock all the nodes in the cluster, then your data is available again\.  If a minimum quorum of healthy nodes doesn't exist, contact AWS Support\.  You can determine the quorum state of your cluster by determining your node's lock state and network reachability\. The `snowballEdge describe-cluster` command reports back the lock and network reachability state for every node in an unlocked cluster\. Ensuring that the appliances in your cluster are healthy and connected is an administrative responsibility that you take on when you create the cluster job\. For more information on the different client commands, see [Commands for the Snowball Client](using-client-commands.md)\.   Considerations for Cluster Jobs for AWS Snowball EdgeCluster Job Considerations  Find a list of considerations for AWS Snowball Edge clustering\.   Keep the following considerations in mind when planning to use a cluster of Snowball Edges:   We recommend that you have a redundant power supply to reduce potential performance and stability issues for your cluster\.   As with standalone local storage and compute jobs, the data stored in a cluster can't be imported into Amazon S3 without ordering additional appliances as a part of separate import jobs\. Then you can transfer the data from the cluster to those appliances and import the data when you return the appliances for the import jobs\.   To get data onto a cluster from Amazon S3, create a separate export job and copy the data from the appliances of the export job onto the cluster\.   You can create a cluster job from the console, the AWS CLI, or one of the AWS SDKs\. For a guided walkthrough of creating a job, see [Getting Started with an AWS Snowball Edge Appliance](getting-started.md)\.   Cluster nodes have node IDs\. A *node ID* is the same as the job ID for a device that you can get from the console, the AWS CLI, the AWS SDKs, and the Snowball client\. You can use node IDs to remove old nodes from clusters\. You can get a list of node IDs by using the `snowballEdge describe-device` command on an unlocked device or the `describe-cluster` on an unlocked cluster\.   The lifespan of a cluster is limited by the security certificate granted to the cluster devices when the cluster is provisioned\. By default, Snowball Edge devices can be used for up to 120 days before they need to be returned\. At the end of that time, the devices stop responding to read/write requests\. If you need to keep one or more devices for longer than 120 days, contact AWS Support\.   When AWS receives a returned appliance that was part of a cluster, we perform a complete erasure of the appliance\. This erasure follows the National Institute of Standards and Technology \(NIST\) 800\-88 standards\.      Related Topics  Links to topics related to clustering in other topics in this guide\.   Beyond the content presented in this topic, you can find other topics in this guide that are relevant to clusters:   [Getting Started with an AWS Snowball Edge Appliance](getting-started.md) – This section outlines how to get started creating your first job\. The techniques in this section work for all job types, including cluster jobs\.   [Commands for the Snowball Client](using-client-commands.md) – This section contains a list of commands for the Snowball client tool\. These commands include the Snowball Edge administrative commands to unlock a cluster, get the status information for the nodes and the cluster as a whole, remove unavailable nodes, and add new nodes\.   [Administrating a Cluster](administercluster.md) – This section contains information about the administrative tasks you perform with a cluster, like adding and removing nodes, and includes helpful procedures\.      Administrating a Cluster  Learn to administer a Snowball Edge cluster\.   Following, you can find information about administrative tasks to operate a healthy cluster of Snowball Edge devices\. The primary administrative tasks are covered in the following topics\.  Most administrative tasks require that you use the Snowball client and its commands that perform the following actions:   [Unlocking AWS Snowball Edge Devices](using-client-commands.md#setting-up-client)   [Getting Device Status](using-client-commands.md#client-status) of a cluster   [Removing a Node from a Cluster](using-client-commands.md#client-cluster-remove)   [Adding a Node to a Cluster](using-client-commands.md#client-cluster-add)    Reading and Writing Data to a Cluster  After you've unlocked a cluster, you're ready to read and write data to it\. You can use the Amazon S3 Adapter for Snowball to read and write data to a cluster\. For more information, see [Using the Amazon S3 Adapter](using-adapter.md)\. To write data to a cluster, you must have a read/write quorum with no more than one unavailable node\. To read data from a cluster, you must have a read quorum of no more than two unavailable nodes\. For more information on quorums, see [Snowball Edge Cluster Quorums](UsingCluster.md#clusterquorums)\.   Reconnecting an Unavailable Cluster Node  A node can become temporarily unavailable due to an issue \(like power or network loss\) without damaging the data on the node\. When this happens, it affects the status of your cluster\. A node's network reachability and lock status is reported in the Snowball client by using the `snowballEdge describe-cluster` command\. We recommend that you physically position your cluster so you have access to the front, back, and top of all nodes\. This way, you can access the power and network cables on the back, the shipping label on the top to get your node ID, and the LCD screen on the front of the device for the IP address and other administrative information\. When you detect that a node is unavailable, we recommend that you try one of the following procedures, depending on the scenario that caused the unavailability\. To reconnect an unavailable node  Ensure that the node has power\.   Ensure that the node is connected to the same internal network that the rest of the cluster is on\.   Wait for the node to finish powering up, if it needed to be powered up\.   Run the `snowballEdge unlock-cluster` command, or the `snowballEdge associate-device` command\. For an example, see [Unlocking AWS Snowball Edge Devices](using-client-commands.md#setting-up-client)\.   To reconnect an unavailable node that lost network but didn't lose power  Ensure that the node is connected to the same internal network that the rest of the cluster is on\.   Run the `snowballEdge describe-device` command to see when the previously unavailable node is added back to the cluster\. For an example, see [Getting Device Status](using-client-commands.md#client-status)\.   When you have performed the preceding procedures, your nodes should be working normally\. You should also have a read/write quorum\. If that's not the case, then one or more of your nodes might have a more serious issue and might need to be removed from the cluster\.   Removing an Unhealthy Node from a Cluster  Rarely, a node in your cluster might become unhealthy\. If the node is unavailable, we recommend going through the procedures listed in [Reconnecting an Unavailable Cluster Node](#reconnectingclusternode) first\.  If doing so doesn't resolve the issue, then the node might be unhealthy\. An unhealthy node can occur if the node has taken damage from an external source, if there was an unusual electrical event, or if some other unlikely event occurs\. If this happens, you need to remove the node from the cluster before you can add a new node as a replacement\. When you detect that a node is unhealthy and needs to be removed, we recommend that you do so with the following procedure\. To remove an unhealthy node  Ensure that the node is unhealthy and not just unavailable\. For more information, see [Reconnecting an Unavailable Cluster Node](#reconnectingclusternode)\.   Disconnect the unhealthy node from the network and power it off\.   Run the `snowballEdge dissassociate-device` Snowball client command\. For more information, see [Removing a Node from a Cluster](using-client-commands.md#client-cluster-remove)\.   Order a replacement node using the console, the AWS CLI, or one of the AWS SDKs\.   Return the unhealthy node to AWS\. When we have the node, we perform a complete erasure of the device\. This erasure follows the National Institute of Standards and Technology \(NIST\) 800\-88 standards\.   After you successfully remove a node, your data is still available on the cluster if you still have a read quorum\. To have read quorum, a cluster must have no more than two unavailable nodes\. Therefore, we recommend that you order replacement nodes as soon as you remove an unavailable node from the cluster\.   Adding or Replacing a Node in a Cluster  You can add a new node after you have removed an unhealthy node from a cluster\. You can also add a new node to increase local storage\.  To add a new node, you first need to order a replacement\. You can order a replacement node from the console, the AWS CLI, or one of the AWS SDKs\. If you're ordering a replacement node from the console, you can order replacements for any job that hasn't been canceled or completed\. To order a replacement node from the console  Sign in to the [AWS Snowball Management Console](https://console.aws.amazon.com/importexport/home?region=us-west-2)\.   Find and choose a job for a node that belongs to the cluster that you created from the Job dashboard\.   For **Actions**, choose **Replace node**\. Doing this opens the final step of the job creation wizard, with all settings identical to how the cluster was originally created\.   Choose **Create job**\.   Your replacement Snowball Edge is now on its way to you\. When it arrives, use the following procedure to add it to your cluster\. To add a replacement node  Position the new node for the cluster such that you have access to the front, back, and top of all nodes\.   Ensure that the node has power\.   Ensure that the node is connected to the same internal network that the rest of the cluster is on\.   Wait for the node to finish powering up, if it needed to be powered on\.   Run the `snowballEdge associate-device` command\. For an example, see [Adding a Node to a Cluster](using-client-commands.md#client-cluster-add)\.     ](UsingCluster.md#UsingCluster.title)\.

For the AWS Snowball service, a cluster is a collective of AWS Snowball Edge appliances, used as a single logical unit, for local storage and compute purposes\.

A cluster offers two primary benefits over a standalone Snowball Edge for local storage and compute purposes:
+ **Increased Durability** – The data stored in a cluster of Snowball Edges enjoys increased data durability over a single device\. In addition, the data on the cluster remains as safe and viable as it was previously, despite possible Snowball Edge outages in the cluster\. Clusters can withstand the loss of two nodes before the data is in danger\. You can also add or replace nodes\.
+ **Increased Storage** – The total available storage is 45 terabytes of data per node in the cluster\. Thus, in a five\-node cluster there's 225 terabytes of available storage space\. In contrast, there's about 82 terabytes of available storage space in a standalone Snowball Edge\. Clusters that have more than five nodes have even more storage space\.

A cluster of Snowball Edge devices is made of nodes\. There are two types of nodes: primary nodes and secondary nodes\. When writing data to a cluster, data is written from your server, across your internal network, and to the primary node of the cluster\. The primary node then writes the data to the secondary nodes\.

The primary node is the leader of the cluster and performs most of the behind\-the\-scenes management of the cluster\. You designate the primary node from among all the nodes of the cluster when you unlock the cluster for the first time\. You can change the primary node if the current one becomes unavailable\.

## Snowball Edge Cluster Quorums<a name="old-clusterquorums"></a>

A quorum represents the minimum number of Snowball Edge devices in a cluster that must be communicating with each other to maintain some level of operation\. There are two levels of quorum for Snowball Edge clusters—a read/write quorum and a read quorum\.

Let's say you've uploaded your data to a cluster of Snowball Edge devices\. With all devices healthy, you have a read/write quorum for your cluster\.

If one of those nodes goes offline, you have reduced the operational capacity of the cluster\. However, you can still read and write to the cluster\. In that sense, with the cluster operating all but one node, the cluster still has a read/write quorum\.

In this same example, if the external power failure took out two of the nodes in your cluster, any additional or ongoing write operations fail\. But any data that was successfully written to the cluster can be accessed and read\. This situation is called a read quorum\.

Finally, in this example, suppose that a third node loses power\. Then the cluster is offline, and the data in the cluster is unavailable\. You might be able fix this, or the data might be permanently lost, depending on the severity of the event\. If it is a temporary external power event, and you can power the three Snowball Edges back on and unlock all the nodes in the cluster, then your data will be available again\.

**Important**  
If a minimum quorum of healthy nodes doesn't exist, contact AWS Support\.

You can check the quorum state of your cluster by running the `snowballEdge status` command\. Ensuring that the appliances in your cluster are healthy and connected is an administrative responsibility that you take on when you create the cluster job\.

## Considerations for Cluster Jobs for AWS Snowball Edge<a name="old-clusterconsiderations"></a>

Keep the following considerations in mind when planning to use a cluster of Snowball Edges:
+ We recommend that you have a redundant power supply to reduce potential performance and stability issues for your cluster\.
+ As with standalone local storage and compute jobs, the data stored in a cluster can't be imported into Amazon S3 without ordering additional appliances as a part of separate import jobs\. Then you could transfer the data from the cluster to those appliances and import the data when you return the appliances for the import jobs\.
+ To get data onto a cluster from Amazon S3, you have to create a separate export job and copy the data from the appliances of the export job onto the cluster\.
+ You can create a cluster job from the console, the AWS CLI, or one of the AWS SDKs\. For a guided walkthrough of creating a job, see [Getting Started with an AWS Snowball Edge Appliance](getting-started.md)\.
+ Cluster nodes have node IDs\. A *node ID* is the same as the job ID for a device that you can get from the console, the AWS CLI, the AWS SDKs, and the Snowball client\. You can use node IDs to remove old nodes from clusters\. You can get a list of node IDs by using the `snowballEdge status` command\.
+ The lifespan of a cluster is limited by the security certificate granted to the cluster devices when the cluster is provisioned\. By default, Snowball Edge devices can be used for up to 120 days before they need to be returned\. At the end of that time, the devices stop responding to read/write requests\. If you need to keep one or more devices for longer than 120 days, contact AWS Support\.
+ When AWS receives a returned appliance that was part of a cluster, we perform a complete erasure of the appliance\. This erasure follows the National Institute of Standards and Technology \(NIST\) 800\-88 standards\.

## Related Topics<a name="old-relatedcluster"></a>

Beyond the content presented in this topic, you can find other topics in this guide that are relevant to clusters:
+ [Getting Started with an AWS Snowball Edge Appliance](getting-started.md) – This section outlines how to get started creating your first job\. The techniques in this section work for all job types, including cluster jobs\.
+ [Commands for the Snowball Client](old-using-client.md#old-using-client-commands) – This section contains a list of commands for the Snowball client tool\. These commands include the Snowball Edge administrative commands to unlock a cluster, get the status information for the nodes and the cluster as a whole, remove unavailable nodes, and add new nodes\.
+ [Administrating a Cluster](old-administercluster.md) – This section contains information about the administrative tasks you perform with a cluster like adding and removing nodes, and includes helpful procedures\.